\chapter{Processamento de Linguagem Natural}
\label{cap:Processamento}

% Um computador, obviamente, está preparado para entender sua própria linguagem,
% como por exemplo, um compilador interpreta linhas de código fonte para gerar um
% programa executável seguindo exatamente o algoritmo utilizado. Por isso, temos o
% termo Natural no Processamento de Linguagem. 

O objetivo da área de Processamento de Linguagem Natural é interpretar e
analisar a linguagem natural, ou seja, a linguagem utilizada pelo seres humanos não
importando se essa é escrita ou falada \cite{manningschutze1999}.

O Processamento de Linguagem Natural é uma área antiga, sendo anterior a
invenção dos computadores modernos. De fato, sua primeira grande aplicação foi
um dicionário desenvolvido no Birkbeck College em Londres no ano de 1948. Por ser
uma área complexa, seus primeiros trabalhos foram notavelmente falhos o que
causou uma hostilidade por parte das agências fundadoras de pesquisas.

Os primeiros pesquisadores eram muitas vezes bilíngues, como por exemplo,
nativos alemães que imigraram para os Estados Unidos. Acreditava-se que pelo
fato destes terem conhecimento de ambas as linguas, Ingles e Alemão, eles teriam
capacidade de desenvolver programas de computadores que efetuariam a tradução das linguas
de modo satisfatório. Uma vez que esses encontraram muitas dificuldades,
ficou claro que o maior problema não era o conhecimento de ambas as
línguas e sim como expressar esse conhecimento na forma de um programa de
computador \cite{history}.

Para fazermos com que um computador seja capaz de interpretar uma
língua, necessitamos antes entender como nós efetuamos essa
interpretação.
Por isso, uma parte considerável do Processamento de Linguagem Natural está apoiado na área de Linguística.

\section{Linguística}

O objetivo da Linguística é compreender como os humanos adquirem, produzem e
entendem as diversas línguas, ou seja, a forma com que conversamos, a nossa
escrita e outras mídias de comunicação \cite{manningschutze1999}.

Na linguagem tanto escrita, como na falada, existem regras que são utilizadas
para estruturar as expressões. Uma série de dificuldades no Processamento de
Linguagem Natural são ocasionadas pelo fato de que as pessoas constantemente
mudam as regras para satisfazerem suas necessidades de comunicação \cite{manningschutze1999}. Uma vez que as regras são
constantemente modificadas pelo loucutor, se torna extremamente difícil a criação de um software ou hardware efetue a interpretação de uma língua. 

% \subsection{Sintaxe e Semântica}
% 
% No seu livro Estruturas Sintáticas, Noam Chomsky cita as seguintes frases
% ``Ideias verdes incolores dormem furiosamente'' e ``Incolores verde ideias dormem
% furiosamente''.
% 
% A primeira frase, do ponto de vista sintático é correta, porém, assim como a
% segunda frase, semânticamente não faz sentido.
% 
% O fato que podemos modificar as regras da lingua de duas formas distintas é
% utilizado como evidência para a separação da sintaxe e semântica na língua.
% \cite{jacksonmoulinier2007}

\section{Métodos de Processamento de Linguagem Natural}

\subsection{Método Simbólico} 
O método simbólico ou racionalista está
baseado no campo da Linguística e faz o uso da manipulação de símbolos,
significados e regras, de um texto. Um exemplo de método simbólico é o método de
Brill \cite{Brill:1992:SRP:974499.974526}.

Por exemplo, no método de Brill a frase ``João pintou a casa de branco'', será
separada em palavras que serão classificadas através de um dicionário pré-definido.

\begin{table}[htb]
\centering
\begin{tabular}{l|l|l|l|l|l|l}
Palavra         & João        & pintou & a      & casa        & de        
& branco
         \\
%Correta: & Substantivo & Verbo  & Artigo & Substantivo & Preposição &
% Substantivo \\
Classificação:   & 			   & Verbo  & Artigo & Substantivo & Preposição & Adjetivo
\end{tabular}
\label{my-label}
\end{table}

Observa-se que algumas palavras não foram
identificadas, como ``João'' ou classificadas de forma errada, como
``branco". Desta forma, o método utiliza-se de outras duas regras para a
classificação inicial.
A primeira classifica todas as palavras desconhecidas que iniciam em maiúsculo como substantivos, por exemplo, a palavra ``João''.

\begin{table}[htb]
\centering
\begin{tabular}{l|l|l|l|l|l|l}
Palavra         & João        & pintou & a      & casa        & de        
& branco
         \\
%Correta: & Substantivo & Verbo  & Artigo & Substantivo & Preposição &
% Substantivo \\
Classificação:   & \textbf{Substantivo} & Verbo  & Artigo & Substantivo &
Preposição & Adjetivo
\end{tabular}
\label{my-label}
\end{table}

Já a segunda regra, atribui para a palavra desconhecida a mesma classificação
de outas palavras que terminam com as mesmas três letras. Por exemplo, supondo que o
método não tivesse encontrado a palavra ``pintou'' no dicionário, ele irá
associar essa palavra a outras com o sufixo ``tou''. Ou seja, ele iria
classificar a palavra ``pintou'' como verbo.

Após essa classificação inicial, o método executa as seguintes regras:
\newpage
\begin{itemize}
   \item Se uma palavra tem a classificação \textbf{A} e está no contexto
  \textbf{C} então a sua classificação deverá ser mudada para \textbf{B}. Por
  exemplo, se uma palavra \textbf{A} é um adjetivo e uma das duas palavras
  anteriores é uma preposição (contexto \textbf{C}), mude para sua
  classificação para substantivo (classificação \textbf{B}).
  
  \[\overbrace{\text{João}}^\text{Substantivo}
  \overbrace{\text{pintou}}^\text{Verbo}
  \overbrace{\text{a}}^\text{Artigo}
  \underbrace{
  \overbrace{\text{casa}}^\text{Substantivo}
  \overbrace{\text{de}}^\text{Preposição}}_\text{Contexto \textbf{C}}
  \underbrace{\overbrace{\text{branco}}^{\textcolor{red}{Adjetivo}}}_\text{Classificação
  \textbf{A}}
 \]
 
  \item Se uma palavra tem a classificação \textbf{A} e tem uma propriedade
  \textbf{P} então a sua classificação deverá ser \textbf{B}. Por exemplo, se
  uma palavra \textbf{A} foi classificada como um adjetivo e essa inicia com
  letra maiúscula (propriedade \textbf{P}), sua classificação deverá ser
  alterada para substantivo (classificação \textbf{B}).
  
  \[\overbrace{\text{Comprei}}^\text{Verbo}
  \overbrace{\text{flores}}^\text{Substantivo}
  \overbrace{\text{para}}^\text{Preposição}
  \underbrace{\overbrace{\textcircled{L}\text{inda}}^{\textcolor{red}{Adjetivo}}}_\text{Classificação
\textbf{A}}
 \]
 
  \item Se uma palavra tem a classificação \textbf{A} e uma palavra com a
  propriedade \textbf{P} está na região \textbf{R}, sua classificação deverá
  ser \textbf{B}. Por exemplo, se uma das duas palavras anteriores (região
  \textbf{R}) iniciam com letra maiúscula (propriedade \textbf{P}), sua
  classificação deverá ser alterada para substantivo (classificação \textbf{B}).
  
   \[\underbrace{\overbrace{\text{João}}^\text{Substantivo}
  \overbrace{\text{adora}}^\text{Verbo}}_\text{Região \textbf{R}}
  \underbrace{\overbrace{\text{L}\text{inda}}^{\textcolor{red}{Adjetivo}}}_\text{Classificação
  \textbf{A}}
 \]
 
  
\end{itemize}

\subsection{Estatístico} 
O método estatístico ou empírico faz uso de grandes
quantidades de texto como páginas da internet, procurando por padrões e
associações a modelos, podendo ou não estarem relacionados com regras sintáticas
ou semânticas. 

Como exemplo, podemos citar a utilização de \textit{Markov Models} com o
algorítmo de Viterbi:

A partir de um banco de dados é verificada a possibilidade de
transição entre as classes gramaticais. Como por exemplo, se no nosso banco de
dados, temos 10000 substantivos que em 7000 dos casos são seguidos por um verbo:

\[ P(VB|SM) = \frac{C(VB,SM)}{C(MD)} = \frac{7000}{10000} = 0.7 \]

Também é verificada a probabilidade de cada palavra ser associada com uma
classe. Supondo que dos 10000 substantivos, 150 são a palavra ``um'':

\[ P(um|SM) = \frac{C(SM,um)}{C(SM)} = \frac{150}{10000} = 0.015 \]

Como exemplo, vamos supor os seguintes dados compilados através dos métodos
citados anteriormente:

\begin{table}[htb]
\centering
\label{my-label}
\begin{tabular}{|l|l|l|l|l|}
\hline
            & Substantivo & Verbo & Artigo & Pronome \\ \hline
Início      & 0.30        & 0.25  & 0.35   & 0.4     \\ \hline
Substantivo & 0.4         & 0.2   & 0.015  & 0.85    \\ \hline
Verbo       & 0.5         & 0.2   & 0.3    & 0.07    \\ \hline
Artigo      & 0.1         & 0.001 & 0.030  & 0.005   \\ \hline
Pronome     & 0.7         & 0.005 & 0.005  & 0.8     \\ \hline
\end{tabular}
\caption{Tabela de Probabilidade de Transição}
\end{table} 

\begin{table}[htb]
\centering
\label{my-label2}
\begin{tabular}{|l|l|l|l|l|}
\hline
            & João & comprou & um    & carro \\ \hline
Substantivo & 1    &         & 0.015 & 1     \\ \hline
Verbo       &      & 1       &       &       \\ \hline
Artigo      &      &         & 0.010 &       \\ \hline
Pronome     &      &         & 0.005 &       \\ \hline
\end{tabular}

\caption{Tabela de Probabilidades de Associação}
\end{table}

Portanto, temos a seguinte imagem representando as possibilidades de
classificação da frase de exemplo:

\begin{figure}[htbp]
 \centering
 \includegraphics[height=180px]{imagens/markov.png}
 \caption{Caminhos possíveis de classificação}
 \label{fig:markov}
\end{figure}

Para identificar qual a classe correta da palavra ``um'' é realizado o seguinte
cálculo:

\[ v_t(j) = v_{t-1} a_{ij} b_j(o_t) \]

Aonde que $v_t$ é a probabilidade do caminho atual, $v_{t-1}$ é a
probabilidade do caminho anterior, $a_{ij}$ é a probabilidade de transição e $b_j(o_t)$ é a
probabilidade de associação.

Portanto para a palavra ``João'':

\[ v_t(j) = 1 * 0.3 * 1 \]

E para ``comprou'':

\[ v_t(j) = 0.3 * 0.2 * 1 \] 

Ao efetuar o cálculo de todos os caminhos, para determinar qual a classificação
correta de uma palavra, é escolhido o caminho que tem maior probabilidade, no
caso apresentado, a palavra ``um'' é classificada como substantivo.

\begin{figure}[htbp]
 \centering
 \includegraphics[height=180px]{imagens/markov2.png}
 \caption{Caminhos já decididos de classificação}
 \label{fig:markov2}
\end{figure}

Como visto, o método simbólico para resolver problemas de Processamento de
Linguagem Natural faz uso da criação de regras baseadas no conhecimento humano,
enquanto o método estatístico, decide através de cálculos probabilísticos
apoiados em estatísticas de um banco de dados para a resolução correta do
problema.


% Uma maneira de diferenciarmos os dois métodos é através do problema de
% ambiguidade. Por exemplo, nas frases:
% 
% ``João entrou no carro conversível de óculos novos.''. E ``João entrou no carro
% conversível de farol apagado.''.
% 
% Em ambas as frases, após a preposição ``de'' segue um substantivo masculino.
% Porém, cada uma das frases se refere a um substantivo diferente. A
% primeira se refere ao João, visto que não existe sentido em um carro ter óculos.
% Já a segunda se refere ao próprio carro, visto que não existe sentido em João
% ter faróis.
% 
% O método simbólico para resolver esse problema faz a criação de novas regras se
% baseadas no conhecimento humano para a solução de qual o significado da frase.
% Já o método estatístico, irá verificar qual a probabilidade de cada significado
% para cada frase através de análises similares decidindo através de metódos
% estatísticos qual o significado correto para cada frase
% \cite{jacksonmoulinier2007}.

\section{Classificadores}
\label{cap:Classificadores}

Para o \ac{NLP} e também para o campo de estatísticas, classificadores são
algorítmos que identificam a qual categoria determinado item pertence. Essa
classificação é feita a partir de dados já classificados corretamente, ou seja,
um \textit{training set}.

\subsection{Naive Bayes}

O classificador Naive Bayes é um classificador baseado no teorema de Bayes com
independencia entre seus atributos.

O teorema de Bayes é representado da seguinte forma:

\[ P(c|d) = \frac{P(d|c) P(c)}{P(d)}  \]

Supondo que precisamos determinar se o carro que João comprou na frase ``João
comprou um Focus.'' é o modelo sedan ou hatch.
 
\begin{itemize}
  \item P(c$\vert$d) é a probabilidade de \textbf{d} pertencer a classe
  \textbf{c}. Ou seja, a probabilidade do carro Focus ser um sedan.
  \item P(d$\vert$c) é a probabilidade da classe \textbf{c} ser \textbf{d}. Ou
  seja, dentre todas as sedans, a probabilidade de um sedan ser
  um Focus.
  \item P(c) é a probabilidade da classe \textbf{c}. Ou seja, a frequência que
  sedans aparecem no nosso banco de dados.
  \item P(d) é a probabilidade de \textbf{d}. Ou seja, a frequência que Focus aparecem no nosso banco de dados.
\end{itemize}

Levando em consideração que temos o banco de dados representado pela tabela
abaixo:

\begin{table}[htb]
\centering
\label{123}
\begin{tabular}{|l|l|}
\hline
Carro  & Categoria \\ \hline
Focus  & Sedan     \\ \hline
Gol    & Hatch     \\ \hline
Focus  & Hatch     \\ \hline
Focus  & Sedan     \\ \hline
Focus  & Hatch     \\ \hline
Fox    & Hatch     \\ \hline
Fiesta & Hatch     \\ \hline
Cruze  & Sedan     \\ \hline
Focus  & Hatch     \\ \hline
\end{tabular} 
\caption{Tabela de Carro e Categoria.}
\end{table}

Probabilidade do Focus ser sedan:

\[ P(Sedan|Focus) = \frac{P(Focus|Sedan) P(Sedan)}{P(Focus)}  \]

\[ P(Sedan|Focus) = \frac{2/3 * 3/10}{5/10} = \frac{0,2}{0,5} = 0,4\]

Probabilidade do Focus ser um hatch:

\[ P(Hatch|Focus) = \frac{P(Focus|Hatch) P(Hatch)}{P(Focus)}  \]

\[ P(Hatch|Focus) = \frac{3/6 * 7/10}{5/10} = \frac{0,35}{0,5} = 0,7\]

No caso utilizado como exemplo, o Focus(Atributo ou \textit{Feature}) é um hatch
(Rótulo ou \textit{Label}).
Porém, caso tenhamos mais um atributo para utilizar na classificação o classificador Naive Bayes não
considera nenhuma dependência.
Como por exemplo a frase ``O carro era ano 2010 e tinha 2 portas'' com os
seguintes atributos:

\begin{table}[htb]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Carro  & Ano  & Portas & Categoria \\ \hline
Focus  & 2010 & 2      & Sedan     \\ \hline
Gol    & 2010 & 4      & Hatch     \\ \hline
Focus  & 2011 & 4      & Hatch     \\ \hline
Focus  & 2011 & 2      & Sedan     \\ \hline
Focus  & 2011 & 2      & Hatch     \\ \hline
Fox    & 2012 & 4      & Hatch     \\ \hline
Fiesta & 2012 & 2      & Hatch     \\ \hline
Cruze  & 2013 & 4      & Sedan     \\ \hline
Focus  & 2013 & 2      & Hatch     \\ \hline
\end{tabular}
\caption{Tabela de anos, carros, portas e categorias.}
\label{my-label}
\end{table}

O cálculo será feito da seguinte forma:

\[ P(d|c) = P(d_1|c) * P(d_2|c)* \ldots * P(d_n|c) \]

\[ P(Sedan|Focus) = P(Portas=2|Sedan) * P(Ano=2010|Sedan) \ldots * \]

\[ P(Sedan|Focus) = 2/3 * 1/3 \ldots * \]

Por assumir independência entre seus atributos, os valores obtidos nós cálculos
podem ser armazenados no banco de dados e reaproveitados. Por isso, sua
performance é considerada incrívelmente boa até mesmo para casos aonde temos forte dependência de
atributos \cite{domingos97naivebayes}.

%TODO EXEMPLO
\subsection{\textit{Maximum Entropy}}
O classificador \ac{MaxEnt} tem como
característica principal a preferência por modelos de dados uniformes sem efetuar nenhuma suposição injustificada.

Podemos utilizar um exemplo similar ao anterior para demonstrar a lógica do
classificador \ac{MaxEnt}. 
 
``João comprou um carro.''.

Supondo que temos que classificar o tipo de carro comprado em três categorias:

\begin{itemize}
  \item Hatch.
  \item Sedan.
  \item Cupê.
\end{itemize}

Podemos afirmar que:

\[ P(Hatch) + P (Sedan) + P(Cupe) = 1 \]

Como só temos essas três possibilidades de classificação no nosso
exemplo, o carro só pode ser classificado em uma dessas três possibilidades, ou
seja, a soma das três probabilidades deve ser 100\% ou 1 essa é a primeira
restrição ou \textit{constraint}. Abaixo duas tabelas que satisfazem essa
restrição:

\begin{table}[htb]
\centering
\begin{tabular}{|l|l|}
\hline
Tipo  & \%   \\ \hline
Sedan & 33\% \\ \hline
Hatch & 33\% \\ \hline
Cupê  & 33\% \\ \hline
 \end{tabular}
\caption{Tabela de Probabilidades A} 
\end{table}
\begin{table}[htb]
\centering
 \begin{tabular}{|l|l|}
\hline
Tipo  & \%   \\ \hline
Sedan & 50\% \\ \hline
Hatch & 50\% \\ \hline
Cupê  & 0\% \\ \hline
 \end{tabular}  
\caption{Tabela de Probabilidades B} 
\end{table}

Sem nenhum conhecimento prévio da distribuição desses carros, ou seja, a
quantidade de carros comprados por tipo, o classificador assume uma distribuição
uniforme das probabilidades, portanto, com maior entropia.

\begin{itemize}
  \item Hatch - 33\%.
  \item Sedan - 33\%.
  \item Cupê - 33\%.
\end{itemize}

Agora, supondo que a partir do nosso banco de dados conseguimos verificar que em
80\% dos casos o veículo comprado era um sedan ou hatch, temos uma nova
restrição:

\[ P(Hatch) + P (Sedan) = 0.8 \]

Podemos novamente ter n distribuições diferentes, porém a distribuição mais
uniforme que satisfaz as nossas duas restrições são:

\begin{itemize}
  \item Hatch - 40\%.
  \item Sedan - 40\%.
  \item Cupê - 30\%.
\end{itemize}

Esse é o princípio da Máxima Entropia utilizado nessa forma de classificação.
Primeiro é descoberta a frequência de cada atributo, depois é procurada a
distribuição que máximiza a entropia, ou seja, a mais uniforme.

%\subsection{Modelo Probabilístico}

%O seu modelo probabilístico para as probabilidades de observarmos o evento c
%quando d for verdadeiro:

%\[ P(c|d) := \frac{1}{Z(d)} exp (\Sigma_i\lambda_{i,c}F_{i,c}(d,c))\]




